{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides a basic illustration of how to use different parts of LegalBench. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "os.environ['VLLM_USE_MODELSCOPE'] = 'True'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '7'\n",
    "from vllm import LLM, SamplingParams\n",
    "prompts = [\n",
    "    \"Hello, my name is\",\n",
    "    \"The president of the United States is\",\n",
    "    \"The capital of France is\",\n",
    "    \"The future of AI is\",\n",
    "]\n",
    "sampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n",
    "llm = LLM(model=\"/home/jye/huggingface/pretrained_model/qwen/Qwen1.5-7B-Chat\", trust_remote_code=True, dtype=torch.float16)\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "#Print the outputs.\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import datasets\n",
    "\n",
    "from tasks import TASKS, ISSUE_TASKS\n",
    "from utils import generate_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supress progress bars which appear every time a task is downloaded\n",
    "datasets.utils.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task organization\n",
    "\n",
    "`tasks.py` provides data structures which organize all LegalBench tasks. For instance, `TASKS` lists all LegalBench tasks, and `ISSUE_TASKS` lists all tasks in the issue-spotting reasoning category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(TASKS), TASKS[:10])\n",
    "print()\n",
    "print(len(ISSUE_TASKS), ISSUE_TASKS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading task data\n",
    "\n",
    "LegalBench can be downloaded from Huggingface: https://huggingface.co/datasets/nguha/legalbench. Each LegalBench dataset comes with `train` and `test` split.\n",
    "\n",
    "- The `train` split is small (usually fewer than 10 samples). Following the [RAFT](https://raft.elicit.org/) benchmark, it's intended to provide labaled samples that can be used as few-shot demonstrations for prompts.\n",
    "- The `test` split is larger, and contains samples to evaluate an LLM on. \n",
    "\n",
    "Documentation for each task can be found on the Github repository, under the task-specific folder. For instance, the documentation for the `abercrombie` task can be found at <https://github.com/HazyResearch/legalbench/tree/main/tasks/abercrombie>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset(\"nguha/legalbench\", \"abercrombie\")\n",
    "dataset[\"train\"].to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and applying prompts\n",
    "\n",
    "Each task folder also stores prompt templates which can be used with different models. In LegalBench, prompt templates are represented as text files, in which \"{{col_name}}\" denote place holders for column names.\n",
    "\n",
    "For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base prompt\n",
    "with open(f\"tasks/abercrombie/base_prompt.txt\") as in_file:\n",
    "    prompt_template = in_file.read()\n",
    "print(prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script `utils.py` provides a simple function for generating prompts for a dataset given a template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = dataset[\"test\"].to_pandas()\n",
    "prompts = generate_prompts(prompt_template=prompt_template, data_df=test_df)\n",
    "print(prompts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Qwen2ForCausalLM, Qwen2Tokenizer\n",
    "\n",
    "model_name_or_path = \"/home/jye/huggingface/pretrained_model/qwen/Qwen1.5-7B-Chat\"\n",
    "\n",
    "tokenizer = Qwen2Tokenizer.from_pretrained(model_name_or_path, )\n",
    "model = Qwen2ForCausalLM.from_pretrained(model_name_or_path, )\n",
    "\n",
    "text = prompts[0]\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_length=100)\n",
    "output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "The majority of LegalBench tasks are evaluated using balanced-accuracy. A handful of tasks which involve extraction or multilabel classification are evaluated using F1. To simplify evaluation, we provide an evaluation which which scores performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation import evaluate\n",
    "import numpy as np\n",
    "\n",
    "# Generate random predictions for abercrombie\n",
    "# Change this, using qwen or other models\n",
    "# classes = [\"generic\", \"descriptive\", \"suggestive\", \"arbitrary\", \"fanciful\"]\n",
    "# generations = np.random.choice(classes, len(test_df))\n",
    "generations = [tokenizer.decode(model.generate(**tokenizer(prompt, return_tensors=\"pt\")), skip_special_tokens=True) for prompt in prompts]\n",
    "\n",
    "generations\n",
    "# evaluate(\"abercrombie\", generations, test_df[\"answer\"].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting tasks by license\n",
    "\n",
    "LegalBench tasks are covered under different licenses. The following code allows you to filter out tasks by license type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_license = \"CC BY 4.0\"\n",
    "tasks_with_target_license = []\n",
    "for task in tqdm(TASKS):\n",
    "    dataset = datasets.load_dataset(\"nguha/legalbench\", task, split=\"train\")\n",
    "    if dataset.info.license == target_license:\n",
    "        tasks_with_target_license.append(task)\n",
    "print(\"Tasks with target license:\", tasks_with_target_license)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
